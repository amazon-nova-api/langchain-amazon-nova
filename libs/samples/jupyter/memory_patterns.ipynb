{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Memory Patterns with Amazon Nova\n",
    "\n",
    "This notebook demonstrates different memory patterns for maintaining conversation context with Nova models.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure you have set your environment variables:\n",
    "```bash\n",
    "export NOVA_API_KEY=\"your-api-key\"\n",
    "export NOVA_BASE_URL=\"https://api.nova.amazon.com/v1\"\n",
    "```"
   ]
  },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {},
  "outputs": [],
  "source": [
   "%env NOVA_API_KEY=\"YOUR-API-KEY\"\n",
   "%env NOVA_BASE_URL=https://api.nova.amazon.com/v1/"
  ]
 },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nova import ChatNova\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatNova(\n",
    "    model=\"nova-pro-v1\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    "    metadata={\"demo\": \"memory_patterns\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Manual Message History\n",
    "\n",
    "The simplest approach: manually manage a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with system message\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that remembers context.\")\n",
    "]\n",
    "\n",
    "# Turn 1: User introduces themselves\n",
    "messages.append(HumanMessage(content=\"Hi! My name is Alice and I'm learning Python.\"))\n",
    "response1 = llm.invoke(messages)\n",
    "messages.append(AIMessage(content=response1.content))\n",
    "\n",
    "print(\"Turn 1:\")\n",
    "print(f\"User: Hi! My name is Alice and I'm learning Python.\")\n",
    "print(f\"Assistant: {response1.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2: Ask about their name (tests memory)\n",
    "messages.append(HumanMessage(content=\"What's my name and what am I learning?\"))\n",
    "response2 = llm.invoke(messages)\n",
    "messages.append(AIMessage(content=response2.content))\n",
    "\n",
    "print(\"Turn 2:\")\n",
    "print(f\"User: What's my name and what am I learning?\")\n",
    "print(f\"Assistant: {response2.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check message history\n",
    "print(f\"Total messages in history: {len(messages)}\")\n",
    "for i, msg in enumerate(messages):\n",
    "    print(f\"{i+1}. {msg.type}: {msg.content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conversation Buffer Memory\n",
    "\n",
    "LangChain's `ConversationBufferMemory` manages message history for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create buffer memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# Add conversations\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hi, I'm Bob and I live in Seattle\"},\n",
    "    {\"output\": \"Hello Bob! It's nice to meet you. Seattle is a beautiful city!\"}\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"I work as a software engineer\"},\n",
    "    {\"output\": \"That's great! Software engineering is an exciting field. What technologies do you work with?\"}\n",
    ")\n",
    "\n",
    "print(f\"Buffer has {len(memory.chat_memory.messages)} messages\")\n",
    "for msg in memory.chat_memory.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use memory in a chain\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"history\": memory.chat_memory.messages,\n",
    "    \"input\": \"Where do I live and what do I do?\"\n",
    "})\n",
    "\n",
    "print(f\"Assistant: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conversation Summary Memory\n",
    "\n",
    "For long conversations, `ConversationSummaryMemory` creates a rolling summary instead of keeping all messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary memory\n",
    "summary_memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
    "\n",
    "# Simulate a longer conversation\n",
    "conversations = [\n",
    "    (\"What are quantum computers?\", \"Quantum computers use quantum bits or qubits that can be in superposition...\"),\n",
    "    (\"How do they differ from classical computers?\", \"Classical computers use binary bits while quantum computers leverage quantum mechanics...\"),\n",
    "    (\"What are practical applications?\", \"Applications include cryptography, drug discovery, optimization problems...\"),\n",
    "    (\"Are they commercially available yet?\", \"Yes, companies like IBM and Google offer cloud access to quantum computers...\"),\n",
    "]\n",
    "\n",
    "for user_msg, ai_msg in conversations:\n",
    "    summary_memory.save_context({\"input\": user_msg}, {\"output\": ai_msg})\n",
    "\n",
    "print(\"Conversation Summary:\")\n",
    "print(summary_memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summary is much shorter than storing all messages\n",
    "print(f\"\\nSummary length: {len(summary_memory.buffer)} characters\")\n",
    "print(f\"Original conversation would be ~{sum(len(q) + len(a) for q, a in conversations)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Context Window Management\n",
    "\n",
    "Practical pattern for managing conversation within token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set maximum messages to keep (system message + N exchanges)\n",
    "MAX_MESSAGES = 10\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a math tutor. Be concise.\")\n",
    "]\n",
    "\n",
    "# Simulate multiple questions\n",
    "questions = [\n",
    "    \"What is 15 + 27?\",\n",
    "    \"What is 8 * 9?\",\n",
    "    \"What is 100 / 4?\",\n",
    "    \"What is 12 squared?\",\n",
    "    \"What is the square root of 144?\",\n",
    "    \"What is 50 - 18?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    messages.append(HumanMessage(content=question))\n",
    "    response = llm.invoke(messages)\n",
    "    messages.append(AIMessage(content=response.content))\n",
    "    \n",
    "    print(f\"Q{i}: {question}\")\n",
    "    print(f\"A{i}: {response.content}\")\n",
    "    \n",
    "    # Trim if exceeding max\n",
    "    if len(messages) > MAX_MESSAGES:\n",
    "        # Keep system message + most recent messages\n",
    "        messages = [messages[0]] + messages[-(MAX_MESSAGES-1):]\n",
    "        print(f\"  [Trimmed to {len(messages)} messages]\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combining Patterns\n",
    "\n",
    "Use buffer memory with a sliding window for the best of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowMemory:\n",
    "    \"\"\"Memory that keeps only the N most recent exchanges.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_exchanges=5):\n",
    "        self.max_exchanges = max_exchanges\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_exchange(self, human_msg: str, ai_msg: str):\n",
    "        self.messages.append(HumanMessage(content=human_msg))\n",
    "        self.messages.append(AIMessage(content=ai_msg))\n",
    "        \n",
    "        # Keep only last N exchanges (2 messages per exchange)\n",
    "        max_msgs = self.max_exchanges * 2\n",
    "        if len(self.messages) > max_msgs:\n",
    "            self.messages = self.messages[-max_msgs:]\n",
    "    \n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "    \n",
    "    def clear(self):\n",
    "        self.messages = []\n",
    "\n",
    "# Test it\n",
    "sliding_memory = SlidingWindowMemory(max_exchanges=3)\n",
    "\n",
    "for i in range(5):\n",
    "    sliding_memory.add_exchange(\n",
    "        f\"This is question number {i+1}\",\n",
    "        f\"This is answer number {i+1}\"\n",
    "    )\n",
    "\n",
    "print(f\"After 5 exchanges, memory contains {len(sliding_memory.get_messages())} messages:\")\n",
    "for msg in sliding_memory.get_messages():\n",
    "    print(f\"  {msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Memory Pattern Comparison:**\n",
    "\n",
    "| Pattern | Best For | Pros | Cons |\n",
    "|---------|----------|------|------|\n",
    "| Manual Message History | Simple use cases | Full control, straightforward | Manual management |\n",
    "| ConversationBufferMemory | Short conversations | Easy to use, complete history | Memory grows unbounded |\n",
    "| ConversationSummaryMemory | Long conversations | Bounded memory size | Loses details, extra LLM calls |\n",
    "| Context Window Management | Token-conscious apps | Predictable memory usage | Recent messages only |\n",
    "| Sliding Window | Production chatbots | Balance of history and efficiency | May lose important context |\n",
    "\n",
    "**When to use each:**\n",
    "- **Manual History**: Prototyping, debugging\n",
    "- **Buffer Memory**: Customer support (few turns)\n",
    "- **Summary Memory**: Research assistants, long sessions\n",
    "- **Sliding Window**: High-volume chatbots\n",
    "- **Combination**: Complex applications needing both detail and efficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
