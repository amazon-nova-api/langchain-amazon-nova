{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL) with Amazon Nova\n",
    "\n",
    "This notebook demonstrates chain composition patterns using LangChain Expression Language.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NOVA_API_KEY=\"YOUR-API-KEY\"\n",
    "%env NOVA_BASE_URL=https://api.nova.amazon.com/v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nova import ChatNova\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatNova(model=\"nova-pro-v1\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Chain\n",
    "\n",
    "The most basic LCEL pattern: `prompt | model | parser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple chain\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Invoke it\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sequential Chain\n",
    "\n",
    "Chain multiple operations in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: translate\n",
    "translate_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate this to {language}: {text}\"\n",
    ")\n",
    "translate_chain = translate_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Second step: summarize\n",
    "summarize_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize in one sentence: {text}\"\n",
    ")\n",
    "summarize_chain = summarize_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test translation\n",
    "translation = translate_chain.invoke({\n",
    "    \"text\": \"LangChain is great for building AI applications\",\n",
    "    \"language\": \"Spanish\"\n",
    "})\n",
    "print(f\"Translation: {translation}\\n\")\n",
    "\n",
    "# Then summarize the translation\n",
    "summary = summarize_chain.invoke({\"text\": translation})\n",
    "print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parallel Execution\n",
    "\n",
    "Run multiple chains in parallel using `RunnableParallel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two different chains\n",
    "joke_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Tell a joke about {topic}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Write a haiku about {topic}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run them in parallel\n",
    "parallel_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "results = parallel_chain.invoke({\"topic\": \"clouds\"})\n",
    "\n",
    "print(\"Joke:\")\n",
    "print(results['joke'])\n",
    "print(\"\\nPoem:\")\n",
    "print(results['poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain with Branching Logic\n",
    "\n",
    "Use `RunnablePassthrough` to create more complex data flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain that analyzes and then elaborates\n",
    "analyze_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze this text in 2-3 words: {text}\"\n",
    ")\n",
    "elaborate_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Original: {original}\\nAnalysis: {analysis}\\n\\nElaborate on the analysis:\"\n",
    ")\n",
    "\n",
    "# Chain that passes through original text while adding analysis\n",
    "chain = (\n",
    "    {\"original\": RunnablePassthrough(), \"analysis\": analyze_prompt | llm | StrOutputParser()}\n",
    "    | elaborate_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"The quick brown fox jumps over the lazy dog\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chain with Fallbacks\n",
    "\n",
    "Add fallback behavior for robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary model\n",
    "primary = ChatNova(model=\"nova-pro-v1\", temperature=0.7)\n",
    "\n",
    "# Fallback model\n",
    "fallback_model = ChatNova(model=\"nova-lite-v1\", temperature=0.7)\n",
    "\n",
    "# Create chain with fallback\n",
    "prompt = ChatPromptTemplate.from_template(\"What is {thing}?\")\n",
    "chain_with_fallback = (\n",
    "    prompt | primary | StrOutputParser()\n",
    ").with_fallbacks([\n",
    "    prompt | fallback_model | StrOutputParser()\n",
    "])\n",
    "\n",
    "result = chain_with_fallback.invoke({\"thing\": \"LangChain\"})\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streaming Chains\n",
    "\n",
    "Stream output from chains token by token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a short story about {topic} in 3 sentences.\"\n",
    ")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Streaming output:\")\n",
    "for chunk in chain.stream({\"topic\": \"a robot learning to paint\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chain Composition with Data Transformation\n",
    "\n",
    "Transform data between chain steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform data\n",
    "def extract_keywords(text: str) -> dict:\n",
    "    words = text.split()\n",
    "    return {\"keywords\": \", \".join(words[:3])}\n",
    "\n",
    "# Chain with transformation\n",
    "keyword_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a title using these keywords: {keywords}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    extract_keywords\n",
    "    | keyword_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"artificial intelligence machine learning deep neural networks\")\n",
    "print(f\"Generated title: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**LCEL Chain Patterns:**\n",
    "\n",
    "| Pattern | Syntax | Use Case |\n",
    "|---------|--------|----------|\n",
    "| Simple Chain | `prompt \\| llm \\| parser` | Basic transformations |\n",
    "| Sequential | Multiple chains in order | Multi-step processing |\n",
    "| Parallel | `RunnableParallel(...)` | Independent concurrent operations |\n",
    "| Branching | `RunnablePassthrough()` | Complex data flows |\n",
    "| Fallbacks | `.with_fallbacks([...])` | Error handling, redundancy |\n",
    "| Streaming | `.stream(...)` | Real-time output |\n",
    "| Transformation | Custom functions in chain | Data preprocessing |\n",
    "\n",
    "**Key Advantages:**\n",
    "- **Composable**: Build complex workflows from simple pieces\n",
    "- **Readable**: Pipeline syntax makes logic clear\n",
    "- **Flexible**: Easy to add/remove/modify steps\n",
    "- **Async-ready**: All chains support async execution\n",
    "- **Debuggable**: Each step can be tested independently"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
