{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation) Patterns with Amazon Nova\n",
    "\n",
    "This notebook demonstrates how to build RAG systems using Amazon Nova.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NOVA_API_KEY=<YOUR-API-KEY>\n",
    "%env NOVA_BASE_URL=https://api.nova.amazon.com/v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_amazon_nova import ChatAmazonNova\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatAmazonNova(model=\"nova-pro-v1\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Documents\n",
    "\n",
    "First, let's create a knowledge base about LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"LangChain is a framework for developing applications powered by language models. It provides tools for prompt management, chains, and agents.\",\n",
    "        metadata={\"source\": \"intro\", \"page\": 1},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LCEL (LangChain Expression Language) is a declarative way to compose chains. It uses the pipe operator to connect components.\",\n",
    "        metadata={\"source\": \"lcel\", \"page\": 2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain supports multiple model providers including OpenAI, Anthropic, and others. It provides a unified interface.\",\n",
    "        metadata={\"source\": \"providers\", \"page\": 3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Retrieval Augmented Generation (RAG) combines retrieval with generation. Documents are retrieved and used as context.\",\n",
    "        metadata={\"source\": \"rag\", \"page\": 4},\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(documents)} documents\")\n",
    "for doc in documents:\n",
    "    print(f\"  - {doc.metadata['source']}: {len(doc.page_content)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Keyword-Based Retrieval\n",
    "\n",
    "Basic retrieval using keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_retriever(query: str, docs: list):\n",
    "    \"\"\"Simple keyword-based retrieval.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    relevant = []\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        if query_words & doc_words:\n",
    "            relevant.append(doc)\n",
    "\n",
    "    return relevant\n",
    "\n",
    "\n",
    "query = \"What is LCEL?\"\n",
    "retrieved = simple_retriever(query, documents)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Retrieved {len(retrieved)} documents:\")\n",
    "for doc in retrieved:\n",
    "    print(f\"  - {doc.metadata['source']}: {doc.page_content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic RAG Chain\n",
    "\n",
    "Combine retrieval with generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"Format documents for context.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Create RAG prompt\n",
    "rag_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Use the following context to answer the question. If the context doesn't contain the answer, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Retrieve and answer\n",
    "query = \"What is LCEL?\"\n",
    "retrieved_docs = simple_retriever(query, documents)\n",
    "context = format_docs(retrieved_docs)\n",
    "\n",
    "chain = rag_prompt | llm | StrOutputParser()\n",
    "answer = chain.invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Chain with LCEL\n",
    "\n",
    "Build a more sophisticated RAG chain using runnables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(query: str, k: int = 2):\n",
    "    \"\"\"Retrieve top k documents by keyword scoring.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "\n",
    "    scored_docs = []\n",
    "    for doc in documents:\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        score = len(query_words & doc_words)\n",
    "        scored_docs.append((score, doc))\n",
    "\n",
    "    scored_docs.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [doc for score, doc in scored_docs[:k]]\n",
    "\n",
    "\n",
    "# RAG chain with runnable\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: format_docs(retrieve_top_k(x[\"question\"])),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = rag_chain.invoke({\"question\": \"How does LangChain support different models?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Query RAG\n",
    "\n",
    "Generate multiple query variations for better retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate alternative queries\n",
    "multi_query_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate 2 different versions of this question:\\n{question}\\n\\nReturn only the questions, one per line.\"\n",
    ")\n",
    "\n",
    "original_query = \"What is LangChain used for?\"\n",
    "\n",
    "queries_result = (multi_query_prompt | llm | StrOutputParser()).invoke(\n",
    "    {\"question\": original_query}\n",
    ")\n",
    "alternative_queries = [q.strip() for q in queries_result.split(\"\\n\") if q.strip()]\n",
    "\n",
    "print(f\"Original: {original_query}\")\n",
    "print(f\"Alternatives:\")\n",
    "for q in alternative_queries:\n",
    "    print(f\"  - {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve for all queries\n",
    "all_retrieved = []\n",
    "for q in [original_query] + alternative_queries[:2]:\n",
    "    all_retrieved.extend(retrieve_top_k(q, k=1))\n",
    "\n",
    "# Deduplicate\n",
    "unique_docs = {doc.metadata[\"source\"]: doc for doc in all_retrieved}.values()\n",
    "\n",
    "print(f\"\\nTotal unique documents retrieved: {len(unique_docs)}\")\n",
    "for doc in unique_docs:\n",
    "    print(f\"  - {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer based on combined context\n",
    "context = format_docs(unique_docs)\n",
    "answer = (rag_prompt | llm | StrOutputParser()).invoke(\n",
    "    {\"context\": context, \"question\": original_query}\n",
    ")\n",
    "\n",
    "print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG with Source Attribution\n",
    "\n",
    "Track which documents contributed to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced prompt with source tracking\n",
    "rag_with_sources_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Use the following numbered context to answer the question. \n",
    "Cite sources using [1], [2], etc.\n",
    "\n",
    "Context:\n",
    "{numbered_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs_with_numbers(docs):\n",
    "    \"\"\"Format documents with numbers for citation.\"\"\"\n",
    "    return \"\\n\\n\".join(f\"[{i + 1}] {doc.page_content}\" for i, doc in enumerate(docs))\n",
    "\n",
    "\n",
    "query = \"What does LCEL stand for?\"\n",
    "retrieved = retrieve_top_k(query, k=2)\n",
    "\n",
    "chain = rag_with_sources_prompt | llm | StrOutputParser()\n",
    "answer = chain.invoke(\n",
    "    {\"numbered_context\": format_docs_with_numbers(retrieved), \"question\": query}\n",
    ")\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {answer}\\n\")\n",
    "print(\"Sources:\")\n",
    "for i, doc in enumerate(retrieved, 1):\n",
    "    print(f\"[{i}] {doc.metadata['source']} (page {doc.metadata['page']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Contextual Compression\n",
    "\n",
    "Filter retrieved documents to only relevant portions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_sentences(doc: Document, query: str) -> Document:\n",
    "    \"\"\"Extract sentences relevant to the query.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    sentences = doc.page_content.split(\". \")\n",
    "\n",
    "    relevant_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence_words = set(sentence.lower().split())\n",
    "        if query_words & sentence_words:\n",
    "            relevant_sentences.append(sentence)\n",
    "\n",
    "    if relevant_sentences:\n",
    "        return Document(\n",
    "            page_content=\". \".join(relevant_sentences), metadata=doc.metadata\n",
    "        )\n",
    "    return doc\n",
    "\n",
    "\n",
    "query = \"What is RAG?\"\n",
    "retrieved = retrieve_top_k(query, k=3)\n",
    "\n",
    "# Compress documents\n",
    "compressed = [extract_relevant_sentences(doc, query) for doc in retrieved]\n",
    "\n",
    "print(\"Before compression:\")\n",
    "print(f\"Total chars: {sum(len(d.page_content) for d in retrieved)}\\n\")\n",
    "\n",
    "print(\"After compression:\")\n",
    "print(f\"Total chars: {sum(len(d.page_content) for d in compressed)}\\n\")\n",
    "\n",
    "for doc in compressed:\n",
    "    print(f\"{doc.metadata['source']}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RAG with Self-Query\n",
    "\n",
    "Let the model determine what to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract search criteria from natural language\n",
    "extract_query_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Extract search keywords from this question. Return only keywords, comma-separated:\\n{question}\"\n",
    ")\n",
    "\n",
    "user_question = \"Tell me about how chains work in LangChain\"\n",
    "\n",
    "# Extract keywords\n",
    "keywords = (extract_query_prompt | llm | StrOutputParser()).invoke(\n",
    "    {\"question\": user_question}\n",
    ")\n",
    "\n",
    "print(f\"User question: {user_question}\")\n",
    "print(f\"Extracted keywords: {keywords}\\n\")\n",
    "\n",
    "# Use keywords for retrieval\n",
    "retrieved = retrieve_top_k(keywords, k=2)\n",
    "context = format_docs(retrieved)\n",
    "\n",
    "# Answer\n",
    "answer = (rag_prompt | llm | StrOutputParser()).invoke(\n",
    "    {\"context\": context, \"question\": user_question}\n",
    ")\n",
    "\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Full RAG Pipeline\n",
    "\n",
    "Complete RAG system with all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"Complete RAG pipeline.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, documents, top_k=2):\n",
    "        self.llm = llm\n",
    "        self.documents = documents\n",
    "        self.top_k = top_k\n",
    "\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        \"\"\"Retrieve relevant documents.\"\"\"\n",
    "        return retrieve_top_k(query, k=self.top_k)\n",
    "\n",
    "    def answer(self, question: str):\n",
    "        \"\"\"Answer question using RAG.\"\"\"\n",
    "        docs = self.retrieve(question)\n",
    "        context = format_docs(docs)\n",
    "\n",
    "        chain = self.prompt | self.llm | StrOutputParser()\n",
    "        answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [doc.metadata for doc in docs],\n",
    "        }\n",
    "\n",
    "\n",
    "# Test the pipeline\n",
    "rag = RAGPipeline(llm, documents)\n",
    "\n",
    "questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"What does LCEL do?\",\n",
    "    \"What providers does LangChain support?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    result = rag.answer(q)\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"Sources: {', '.join(s['source'] for s in result['sources'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**RAG Pattern Comparison:**\n",
    "\n",
    "| Pattern | Complexity | Use Case |\n",
    "|---------|------------|----------|\n",
    "| Simple Keyword | Low | Small document sets, prototyping |\n",
    "| Basic RAG Chain | Medium | Most applications |\n",
    "| Multi-Query RAG | Medium | Better recall, diverse queries |\n",
    "| Source Attribution | Medium | Transparency, verification |\n",
    "| Contextual Compression | High | Large documents, token efficiency |\n",
    "| Self-Query | High | Natural language interfaces |\n",
    "| Full Pipeline | High | Production systems |\n",
    "\n",
    "**Key Components:**\n",
    "1. **Documents**: Structured knowledge with metadata\n",
    "2. **Retrieval**: Find relevant documents (keyword, semantic, hybrid)\n",
    "3. **Context Formatting**: Prepare retrieved docs for LLM\n",
    "4. **Prompt Engineering**: Instruct model to use context\n",
    "5. **Generation**: LLM produces answer from context\n",
    "\n",
    "**Best Practices:**\n",
    "- **Chunk Wisely**: Balance between context and specificity\n",
    "- **Include Metadata**: Source tracking for attribution\n",
    "- **Score Retrieval**: Rank by relevance\n",
    "- **Validate Context**: Ensure retrieved docs are actually relevant\n",
    "- **Handle Missing Info**: Model should admit when answer isn't in context\n",
    "\n",
    "**Next Steps:**\n",
    "For production RAG systems, consider:\n",
    "- Vector embeddings for semantic search\n",
    "- Hybrid search (keyword + semantic)\n",
    "- Reranking retrieved documents\n",
    "- Caching for performance\n",
    "- Evaluation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
