{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation) Patterns with Amazon Nova\n",
    "\n",
    "This notebook demonstrates how to build RAG systems using Amazon Nova.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NOVA_API_KEY=<YOUR-API-KEY>\n",
    "%env NOVA_BASE_URL=https://api.nova.amazon.com/v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_amazon_nova import ChatAmazonNova\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatAmazonNova(model=\"nova-pro-v1\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Documents\n",
    "\n",
    "First, let's create a knowledge base about LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4 documents\n",
      "  - intro: 141 chars\n",
      "  - lcel: 125 chars\n",
      "  - providers: 117 chars\n",
      "  - rag: 117 chars\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"LangChain is a framework for developing applications powered by language models. It provides tools for prompt management, chains, and agents.\",\n",
    "        metadata={\"source\": \"intro\", \"page\": 1},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LCEL (LangChain Expression Language) is a declarative way to compose chains. It uses the pipe operator to connect components.\",\n",
    "        metadata={\"source\": \"lcel\", \"page\": 2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain supports multiple model providers including OpenAI, Anthropic, and others. It provides a unified interface.\",\n",
    "        metadata={\"source\": \"providers\", \"page\": 3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Retrieval Augmented Generation (RAG) combines retrieval with generation. Documents are retrieved and used as context.\",\n",
    "        metadata={\"source\": \"rag\", \"page\": 4},\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(documents)} documents\")\n",
    "for doc in documents:\n",
    "    print(f\"  - {doc.metadata['source']}: {len(doc.page_content)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Keyword-Based Retrieval\n",
    "\n",
    "Basic retrieval using keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What is LCEL?'\n",
      "Retrieved 2 documents:\n",
      "  - intro: LangChain is a framework for developing applications powered...\n",
      "  - lcel: LCEL (LangChain Expression Language) is a declarative way to...\n"
     ]
    }
   ],
   "source": [
    "def simple_retriever(query: str, docs: list):\n",
    "    \"\"\"Simple keyword-based retrieval.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    relevant = []\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        if query_words & doc_words:\n",
    "            relevant.append(doc)\n",
    "\n",
    "    return relevant\n",
    "\n",
    "\n",
    "query = \"What is LCEL?\"\n",
    "retrieved = simple_retriever(query, documents)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Retrieved {len(retrieved)} documents:\")\n",
    "for doc in retrieved:\n",
    "    print(f\"  - {doc.metadata['source']}: {doc.page_content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic RAG Chain\n",
    "\n",
    "Combine retrieval with generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LCEL?\n",
      "Answer: LCEL (LangChain Expression Language) is a declarative way to compose chains within the LangChain framework. It utilizes the pipe operator to connect various components, allowing for a more intuitive and streamlined approach to building applications powered by language models.\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"Format documents for context.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Create RAG prompt\n",
    "rag_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Use the following context to answer the question. If the context doesn't contain the answer, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Retrieve and answer\n",
    "query = \"What is LCEL?\"\n",
    "retrieved_docs = simple_retriever(query, documents)\n",
    "context = format_docs(retrieved_docs)\n",
    "\n",
    "chain = rag_prompt | llm | StrOutputParser()\n",
    "answer = chain.invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Chain with LCEL\n",
    "\n",
    "Build a more sophisticated RAG chain using runnables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain supports different models by providing a unified interface that allows developers to interact with multiple model providers, including but not limited to, OpenAI and Anthropic. This abstraction enables developers to switch between different models or providers without needing to change the underlying code significantly, thus promoting flexibility and ease of integration.\n"
     ]
    }
   ],
   "source": [
    "def retrieve_top_k(query: str, k: int = 2):\n",
    "    \"\"\"Retrieve top k documents by keyword scoring.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "\n",
    "    scored_docs = []\n",
    "    for doc in documents:\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        score = len(query_words & doc_words)\n",
    "        scored_docs.append((score, doc))\n",
    "\n",
    "    scored_docs.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [doc for score, doc in scored_docs[:k]]\n",
    "\n",
    "\n",
    "# RAG chain with runnable\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: format_docs(retrieve_top_k(x[\"question\"])),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = rag_chain.invoke({\"question\": \"How does LangChain support different models?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Query RAG\n",
    "\n",
    "Generate multiple query variations for better retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: What is LangChain used for?\n",
      "Alternatives:\n",
      "  - 1. Can you explain the primary purposes and applications of LangChain in various contexts?\n",
      "  - 2. How is LangChain utilized in different industries and what specific problems does it solve?\n"
     ]
    }
   ],
   "source": [
    "# Generate alternative queries\n",
    "multi_query_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate 2 different versions of this question:\\n{question}\\n\\nReturn only the questions, one per line.\"\n",
    ")\n",
    "\n",
    "original_query = \"What is LangChain used for?\"\n",
    "\n",
    "queries_result = (multi_query_prompt | llm | StrOutputParser()).invoke(\n",
    "    {\"question\": original_query}\n",
    ")\n",
    "alternative_queries = [q.strip() for q in queries_result.split(\"\\n\") if q.strip()]\n",
    "\n",
    "print(f\"Original: {original_query}\")\n",
    "print(f\"Alternatives:\")\n",
    "for q in alternative_queries:\n",
    "    print(f\"  - {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique documents retrieved: 1\n",
      "  - intro\n"
     ]
    }
   ],
   "source": [
    "# Retrieve for all queries\n",
    "all_retrieved = []\n",
    "for q in [original_query] + alternative_queries[:2]:\n",
    "    all_retrieved.extend(retrieve_top_k(q, k=1))\n",
    "\n",
    "# Deduplicate\n",
    "unique_docs = {doc.metadata[\"source\"]: doc for doc in all_retrieved}.values()\n",
    "\n",
    "print(f\"\\nTotal unique documents retrieved: {len(unique_docs)}\")\n",
    "for doc in unique_docs:\n",
    "    print(f\"  - {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: LangChain is used for developing applications powered by language models. It provides tools for prompt management, chains, and agents.\n"
     ]
    }
   ],
   "source": [
    "# Answer based on combined context\n",
    "context = format_docs(unique_docs)\n",
    "answer = (rag_prompt | llm | StrOutputParser()).invoke(\n",
    "    {\"context\": context, \"question\": original_query}\n",
    ")\n",
    "\n",
    "print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG with Source Attribution\n",
    "\n",
    "Track which documents contributed to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What does LCEL stand for?\n",
      "Answer: LCEL stands for LangChain Expression Language. It is a declarative way to compose chains, utilizing the pipe operator to connect components [1].\n",
      "\n",
      "Sources:\n",
      "[1] lcel (page 2)\n",
      "[2] intro (page 1)\n"
     ]
    }
   ],
   "source": [
    "# Enhanced prompt with source tracking\n",
    "rag_with_sources_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Use the following numbered context to answer the question. \n",
    "Cite sources using [1], [2], etc.\n",
    "\n",
    "Context:\n",
    "{numbered_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs_with_numbers(docs):\n",
    "    \"\"\"Format documents with numbers for citation.\"\"\"\n",
    "    return \"\\n\\n\".join(f\"[{i + 1}] {doc.page_content}\" for i, doc in enumerate(docs))\n",
    "\n",
    "\n",
    "query = \"What does LCEL stand for?\"\n",
    "retrieved = retrieve_top_k(query, k=2)\n",
    "\n",
    "chain = rag_with_sources_prompt | llm | StrOutputParser()\n",
    "answer = chain.invoke(\n",
    "    {\"numbered_context\": format_docs_with_numbers(retrieved), \"question\": query}\n",
    ")\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {answer}\\n\")\n",
    "print(\"Sources:\")\n",
    "for i, doc in enumerate(retrieved, 1):\n",
    "    print(f\"[{i}] {doc.metadata['source']} (page {doc.metadata['page']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Contextual Compression\n",
    "\n",
    "Filter retrieved documents to only relevant portions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before compression:\n",
      "Total chars: 383\n",
      "\n",
      "After compression:\n",
      "Total chars: 271\n",
      "\n",
      "intro: LangChain is a framework for developing applications powered by language models\n",
      "lcel: LCEL (LangChain Expression Language) is a declarative way to compose chains\n",
      "providers: LangChain supports multiple model providers including OpenAI, Anthropic, and others. It provides a unified interface.\n"
     ]
    }
   ],
   "source": [
    "def extract_relevant_sentences(doc: Document, query: str) -> Document:\n",
    "    \"\"\"Extract sentences relevant to the query.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    sentences = doc.page_content.split(\". \")\n",
    "\n",
    "    relevant_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence_words = set(sentence.lower().split())\n",
    "        if query_words & sentence_words:\n",
    "            relevant_sentences.append(sentence)\n",
    "\n",
    "    if relevant_sentences:\n",
    "        return Document(\n",
    "            page_content=\". \".join(relevant_sentences), metadata=doc.metadata\n",
    "        )\n",
    "    return doc\n",
    "\n",
    "\n",
    "query = \"What is RAG?\"\n",
    "retrieved = retrieve_top_k(query, k=3)\n",
    "\n",
    "# Compress documents\n",
    "compressed = [extract_relevant_sentences(doc, query) for doc in retrieved]\n",
    "\n",
    "print(\"Before compression:\")\n",
    "print(f\"Total chars: {sum(len(d.page_content) for d in retrieved)}\\n\")\n",
    "\n",
    "print(\"After compression:\")\n",
    "print(f\"Total chars: {sum(len(d.page_content) for d in compressed)}\\n\")\n",
    "\n",
    "for doc in compressed:\n",
    "    print(f\"{doc.metadata['source']}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RAG with Self-Query\n",
    "\n",
    "Let the model determine what to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User question: Tell me about how chains work in LangChain\n",
      "Extracted keywords: chains, work, LangChain\n",
      "\n",
      "Answer: In LangChain, \"chains\" are a fundamental component that allows developers to create complex workflows by linking together multiple language model calls and other operations. Here's a detailed explanation of how chains work in LangChain:\n",
      "\n",
      "### What are Chains?\n",
      "Chains in LangChain are sequences of steps where each step can be a call to a language model, a utility function, or another chain. They enable the construction of more sophisticated applications by orchestrating multiple tasks in a specific order.\n",
      "\n",
      "### Key Components of Chains:\n",
      "1. **Prompts**: These are the inputs that you provide to the language model. Chains often start with a prompt that is sent to a language model to generate an initial response.\n",
      "   \n",
      "2. **Language Model Calls**: These are the interactions with the language models (e.g., OpenAI, Anthropic) where the model generates text based on the given prompt.\n",
      "\n",
      "3. **Transformations**: After receiving a response from the language model, you might want to transform this response in some way. This could involve filtering, summarization, or any other form of processing.\n",
      "\n",
      "4. **Conditional Logic**: Chains can incorporate conditional logic to decide the next step based on the output of previous steps. For example, if the model’s response meets certain criteria, the chain might proceed one way; otherwise, it might take a different path.\n",
      "\n",
      "5. **Output**: The final step in a chain usually involves producing an output that can be used by the application. This could be a final text response, a decision, or any other form of result.\n",
      "\n",
      "### Example Workflow:\n",
      "1. **Initial Prompt**: You start with a prompt, such as \"Generate a summary of the following text: [input text].\"\n",
      "2. **Model Call**: The prompt is sent to a language model, which generates a summary.\n",
      "3. **Transformation**: The summary might then be sent through a transformation step, such as checking for coherence or adjusting the tone.\n",
      "4. **Conditional Logic**: Based on the transformed summary, the chain might decide to either output the summary or ask the model to generate a new one if it doesn’t meet certain quality standards.\n",
      "5. **Final Output**: The chain concludes by outputting the final summary to the application.\n",
      "\n",
      "### Benefits of Using Chains:\n",
      "- **Modularity**: Chains allow you to break down complex tasks into smaller, manageable steps.\n",
      "- **Reusability**: Individual components of a chain can be reused in different workflows.\n",
      "- **Flexibility**: Chains can be easily modified or extended to accommodate new requirements.\n",
      "\n",
      "In summary, chains in LangChain provide a powerful way to build intricate applications by sequencing multiple operations and language model interactions in a coherent manner.\n"
     ]
    }
   ],
   "source": [
    "# Extract search criteria from natural language\n",
    "extract_query_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Extract search keywords from this question. Return only keywords, comma-separated:\\n{question}\"\n",
    ")\n",
    "\n",
    "user_question = \"Tell me about how chains work in LangChain\"\n",
    "\n",
    "# Extract keywords\n",
    "keywords = (extract_query_prompt | llm | StrOutputParser()).invoke(\n",
    "    {\"question\": user_question}\n",
    ")\n",
    "\n",
    "print(f\"User question: {user_question}\")\n",
    "print(f\"Extracted keywords: {keywords}\\n\")\n",
    "\n",
    "# Use keywords for retrieval\n",
    "retrieved = retrieve_top_k(keywords, k=2)\n",
    "context = format_docs(retrieved)\n",
    "\n",
    "# Answer\n",
    "answer = (rag_prompt | llm | StrOutputParser()).invoke(\n",
    "    {\"context\": context, \"question\": user_question}\n",
    ")\n",
    "\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Full RAG Pipeline\n",
    "\n",
    "Complete RAG system with all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is LangChain?\n",
      "A: LangChain is a comprehensive framework designed to facilitate the development of applications that leverage language models. It offers a variety of tools and abstractions to simplify the process of integrating and utilizing language models in various applications. Here are the key components and features of LangChain:\n",
      "\n",
      "### Key Components of LangChain\n",
      "\n",
      "1. **Prompt Management:**\n",
      "   - LangChain provides tools for creating, managing, and optimizing prompts that are used to interact with language models. This includes templated prompts, few-shot examples, and dynamic prompt generation.\n",
      "\n",
      "2. **Chains:**\n",
      "   - Chains in LangChain are sequences of steps where the output of one step is the input to the next. This allows for complex workflows to be built by chaining together simpler components. Chains can include tasks like summarization, question-answering, and more.\n",
      "\n",
      "3. **Agents:**\n",
      "   - Agents are more advanced constructs that can make decisions based on the output of chains. They can interact with external tools and APIs, allowing for dynamic and context-aware responses. Agents can plan, execute, and adapt based on the situation.\n",
      "\n",
      "4. **Memory:**\n",
      "   - LangChain includes mechanisms for maintaining context and state across interactions. This is crucial for applications that require conversation history or session-based information.\n",
      "\n",
      "5. **Integrations:**\n",
      "   - The framework supports integration with various language models, including popular ones like OpenAI’s GPT models, as well as other APIs and data sources.\n",
      "\n",
      "### LCEL (LangChain Expression Language)\n",
      "\n",
      "- **Declarative Composition:**\n",
      "  - LCEL is a part of LangChain that allows developers to compose chains in a declarative manner. It uses the pipe operator (`|`) to connect different components, making it easier to read and maintain complex workflows.\n",
      "\n",
      "### Use Cases\n",
      "\n",
      "- **Chatbots and Virtual Assistants:**\n",
      "  - Building interactive and context-aware chatbots that can handle complex conversations.\n",
      "  \n",
      "- **Automated Content Generation:**\n",
      "  - Creating systems that can generate text, summaries, or reports based on input data.\n",
      "  \n",
      "- **Information Retrieval:**\n",
      "  - Developing systems that can fetch and synthesize information from various sources to answer queries.\n",
      "  \n",
      "- **Decision Support Systems:**\n",
      "  - Building agents that can make recommendations or decisions based on data and user input.\n",
      "\n",
      "### Benefits\n",
      "\n",
      "- **Modularity:**\n",
      "  - LangChain’s design promotes modularity, allowing developers to mix and match components to build custom solutions.\n",
      "  \n",
      "- **Scalability:**\n",
      "  - The framework is designed to scale, supporting both simple prototypes and large-scale applications.\n",
      "  \n",
      "- **Extensibility:**\n",
      "  - LangChain is extensible, allowing developers to add new components and integrations as needed.\n",
      "\n",
      "In summary, LangChain is a powerful framework that simplifies the development of language model-powered applications by providing tools for prompt management, chaining components, and building intelligent agents.\n",
      "Sources: intro, lcel\n",
      "\n",
      "Q: What does LCEL do?\n",
      "A: LCEL (LangChain Expression Language) serves as a declarative method for composing and connecting various components within the LangChain framework. Here’s a detailed breakdown of what LCEL does:\n",
      "\n",
      "### Key Functions of LCEL:\n",
      "\n",
      "1. **Declarative Composition**:\n",
      "   - LCEL allows developers to declaratively specify how different components (such as prompts, models, and utilities) should be connected to form a chain. This makes the code more readable and easier to maintain.\n",
      "\n",
      "2. **Use of Pipe Operator**:\n",
      "   - LCEL utilizes the pipe operator (`|`) to chain together different components. This operator signifies the flow of data from one component to the next, making the sequence of operations clear and intuitive.\n",
      "\n",
      "3. **Flexibility and Modularity**:\n",
      "   - By using LCEL, developers can easily modify or extend chains without rewriting large portions of code. Components can be added, removed, or rearranged with minimal effort.\n",
      "\n",
      "4. **Integration with LangChain Components**:\n",
      "   - LCEL works seamlessly with various LangChain components, including prompt templates, language models, and utility functions. This integration enables the creation of complex workflows that leverage the strengths of each component.\n",
      "\n",
      "5. **Enhanced Readability**:\n",
      "   - The declarative nature of LCEL improves the readability of the code. Developers can quickly understand the sequence of operations and the overall structure of the chain.\n",
      "\n",
      "6. **Reusability**:\n",
      "   - Components created using LCEL can be reused across different parts of an application, promoting code reuse and reducing duplication.\n",
      "\n",
      "### Example Usage:\n",
      "\n",
      "```python\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.llms import SomeLanguageModel\n",
      "from langchain.utilities import SomeUtility\n",
      "\n",
      "# Define components\n",
      "prompt_template = PromptTemplate(template=\"Translate the following text: {text}\")\n",
      "language_model = SomeLanguageModel()\n",
      "utility_function = SomeUtility()\n",
      "\n",
      "# Create a chain using LCEL\n",
      "chain = prompt_template | language_model | utility_function\n",
      "\n",
      "# Execute the chain\n",
      "result = chain.run(text=\"Hello, world!\")\n",
      "print(result)\n",
      "```\n",
      "\n",
      "In this example:\n",
      "- `prompt_template` is a PromptTemplate that formats input text.\n",
      "- `language_model` is a language model that processes the formatted text.\n",
      "- `utility_function` is a utility that performs some post-processing on the model’s output.\n",
      "- The pipe operator `|` connects these components in a sequential manner, forming a chain.\n",
      "\n",
      "### Summary:\n",
      "LCEL simplifies the process of composing chains in LangChain by providing a declarative syntax that is both intuitive and flexible. It enhances code readability, promotes reusability, and integrates seamlessly with LangChain’s ecosystem of components.\n",
      "Sources: lcel, intro\n",
      "\n",
      "Q: What providers does LangChain support?\n",
      "A: LangChain supports multiple model providers, including:\n",
      "\n",
      "1. **OpenAI**: Offers models like GPT-3.5, GPT-4, and others.\n",
      "2. **Anthropic**: Provides Claude models.\n",
      "3. **Hugging Face**: Offers a wide range of open-source models.\n",
      "4. **Cohere**: Provides powerful language models for various applications.\n",
      "5. **Google**: Includes models like PaLM and others through Google AI.\n",
      "6. **Amazon**: Offers Bedrock, which provides access to various foundation models.\n",
      "7. **Azure**: Provides access to OpenAI models through Azure Cognitive Services.\n",
      "\n",
      "LangChain provides a unified interface to interact with these diverse model providers, allowing developers to switch between them seamlessly.\n",
      "Sources: providers, intro\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"Complete RAG pipeline.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, documents, top_k=2):\n",
    "        self.llm = llm\n",
    "        self.documents = documents\n",
    "        self.top_k = top_k\n",
    "\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        \"\"\"Retrieve relevant documents.\"\"\"\n",
    "        return retrieve_top_k(query, k=self.top_k)\n",
    "\n",
    "    def answer(self, question: str):\n",
    "        \"\"\"Answer question using RAG.\"\"\"\n",
    "        docs = self.retrieve(question)\n",
    "        context = format_docs(docs)\n",
    "\n",
    "        chain = self.prompt | self.llm | StrOutputParser()\n",
    "        answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [doc.metadata for doc in docs],\n",
    "        }\n",
    "\n",
    "\n",
    "# Test the pipeline\n",
    "rag = RAGPipeline(llm, documents)\n",
    "\n",
    "questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"What does LCEL do?\",\n",
    "    \"What providers does LangChain support?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    result = rag.answer(q)\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"Sources: {', '.join(s['source'] for s in result['sources'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**RAG Pattern Comparison:**\n",
    "\n",
    "| Pattern | Complexity | Use Case |\n",
    "|---------|------------|----------|\n",
    "| Simple Keyword | Low | Small document sets, prototyping |\n",
    "| Basic RAG Chain | Medium | Most applications |\n",
    "| Multi-Query RAG | Medium | Better recall, diverse queries |\n",
    "| Source Attribution | Medium | Transparency, verification |\n",
    "| Contextual Compression | High | Large documents, token efficiency |\n",
    "| Self-Query | High | Natural language interfaces |\n",
    "| Full Pipeline | High | Production systems |\n",
    "\n",
    "**Key Components:**\n",
    "1. **Documents**: Structured knowledge with metadata\n",
    "2. **Retrieval**: Find relevant documents (keyword, semantic, hybrid)\n",
    "3. **Context Formatting**: Prepare retrieved docs for LLM\n",
    "4. **Prompt Engineering**: Instruct model to use context\n",
    "5. **Generation**: LLM produces answer from context\n",
    "\n",
    "**Best Practices:**\n",
    "- **Chunk Wisely**: Balance between context and specificity\n",
    "- **Include Metadata**: Source tracking for attribution\n",
    "- **Score Retrieval**: Rank by relevance\n",
    "- **Validate Context**: Ensure retrieved docs are actually relevant\n",
    "- **Handle Missing Info**: Model should admit when answer isn't in context\n",
    "\n",
    "**Next Steps:**\n",
    "For production RAG systems, consider:\n",
    "- Vector embeddings for semantic search\n",
    "- Hybrid search (keyword + semantic)\n",
    "- Reranking retrieved documents\n",
    "- Caching for performance\n",
    "- Evaluation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
