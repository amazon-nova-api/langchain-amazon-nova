{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Memory Patterns with Amazon Nova\n",
    "\n",
    "This notebook demonstrates modern memory patterns for maintaining conversation context with Nova models.\n",
    "\n",
    "**For current recommendations, see:**\n",
    "- [Short-term memory](https://python.langchain.com/docs/how_to/chatbots_memory/)\n",
    "- [Long-term memory](https://python.langchain.com/docs/how_to/chatbots_long_term_memory/)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure you have set your environment variables:\n",
    "```bash\n",
    "export NOVA_API_KEY=your-api-key\n",
    "export NOVA_BASE_URL=\"https://api.nova.amazon.com/v1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NOVA_API_KEY=<YOUR-API-KEY>\n",
    "%env NOVA_BASE_URL=https://api.nova.amazon.com/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_amazon_nova import ChatAmazonNova\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatAmazonNova(\n",
    "    model=\"nova-pro-v1\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Manual Message History\n",
    "\n",
    "The simplest approach: manually manage a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1:\n",
      "User: Hi! My name is Alice and I'm learning Python.\n",
      "Assistant: Hello, Alice! It's great to meet you, and I'm glad to hear you're learning Python. It's a very versatile and widely-used programming language. What specific areas of Python are you focusing on, or do you have any particular projects or questions in mind? \n",
      "\n",
      "If you need help with any concepts, exercises, or projects, feel free to ask!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start with system message\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that remembers context.\")\n",
    "]\n",
    "\n",
    "# Turn 1: User introduces themselves\n",
    "messages.append(HumanMessage(content=\"Hi! My name is Alice and I'm learning Python.\"))\n",
    "response1 = llm.invoke(messages)\n",
    "messages.append(AIMessage(content=response1.content))\n",
    "\n",
    "print(\"Turn 1:\")\n",
    "print(f\"User: Hi! My name is Alice and I'm learning Python.\")\n",
    "print(f\"Assistant: {response1.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 2:\n",
      "User: What's my name and what am I learning?\n",
      "Assistant: Hello again, Alice! Your name is Alice, and you're learning Python. If you have any questions or need further assistance with your Python studies, just let me know!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Turn 2: Ask about their name (tests memory)\n",
    "messages.append(HumanMessage(content=\"What's my name and what am I learning?\"))\n",
    "response2 = llm.invoke(messages)\n",
    "messages.append(AIMessage(content=response2.content))\n",
    "\n",
    "print(\"Turn 2:\")\n",
    "print(f\"User: What's my name and what am I learning?\")\n",
    "print(f\"Assistant: {response2.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total messages in history: 5\n",
      "1. system: You are a helpful assistant that remembers context....\n",
      "2. human: Hi! My name is Alice and I'm learning Python....\n",
      "3. ai: Hello, Alice! It's great to meet you, and I'm glad to hear y...\n",
      "4. human: What's my name and what am I learning?...\n",
      "5. ai: Hello again, Alice! Your name is Alice, and you're learning ...\n"
     ]
    }
   ],
   "source": [
    "# Check message history\n",
    "print(f\"Total messages in history: {len(messages)}\")\n",
    "for i, msg in enumerate(messages):\n",
    "    print(f\"{i + 1}. {msg.type}: {msg.content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RunnableWithMessageHistory (Recommended)\n",
    "\n",
    "Modern approach using `RunnableWithMessageHistory` for session-based conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session store\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi, I'm Bob from Seattle\n",
      "Assistant: Hello, Bob! It's nice to meet you. Seattle is a great city with a lot to offer, from its beautiful landscapes and proximity to the mountains and water to its vibrant culture and tech scene. Is there something specific youâ€™re interested in or need help with today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First conversation\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi, I'm Bob from Seattle\")]},\n",
    "    config={\"configurable\": {\"session_id\": \"user_123\"}},\n",
    ")\n",
    "\n",
    "print(f\"User: Hi, I'm Bob from Seattle\")\n",
    "print(f\"Assistant: {response.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's my name and where am I from?\n",
      "Assistant: You mentioned that your name is Bob and you're from Seattle. If you have any more questions or need further information, feel free to ask!\n",
      "\n",
      "Session has 4 messages\n"
     ]
    }
   ],
   "source": [
    "# Second conversation - remembers context\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's my name and where am I from?\")]},\n",
    "    config={\"configurable\": {\"session_id\": \"user_123\"}},\n",
    ")\n",
    "\n",
    "print(f\"User: What's my name and where am I from?\")\n",
    "print(f\"Assistant: {response.content}\\n\")\n",
    "\n",
    "# Check session history\n",
    "history = get_session_history(\"user_123\")\n",
    "print(f\"Session has {len(history.messages)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Message Trimming (Context Window Management)\n",
    "\n",
    "Use `trim_messages` to manage the context window and stay within token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What is 15 + 27? -> A1: 15 + 27 = 42\n",
      "Q2: What is 8 * 9? -> A2: 8 * 9 = 72\n",
      "Q3: What is 100 / 4? -> A3: 100 / 4 = 25\n",
      "Q4: What is 12 squared? -> A4: 12 squared = 12 * 12 = 144\n",
      "Q5: What is the square root of 144? -> A5: The square root of 144 is 12.\n",
      "Q6: What is 50 - 18? -> A6: 50 - 18 = 32\n",
      "\n",
      "Total messages: 13\n"
     ]
    }
   ],
   "source": [
    "messages = [SystemMessage(content=\"You are a math tutor. Be concise.\")]\n",
    "\n",
    "# Simulate multiple questions\n",
    "questions = [\n",
    "    \"What is 15 + 27?\",\n",
    "    \"What is 8 * 9?\",\n",
    "    \"What is 100 / 4?\",\n",
    "    \"What is 12 squared?\",\n",
    "    \"What is the square root of 144?\",\n",
    "    \"What is 50 - 18?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    messages.append(HumanMessage(content=question))\n",
    "    response = llm.invoke(messages)\n",
    "    messages.append(AIMessage(content=response.content))\n",
    "    print(f\"Q{i}: {question} -> A{i}: {response.content}\")\n",
    "\n",
    "print(f\"\\nTotal messages: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 13 messages\n",
      "Trimmed: 5 messages\n",
      "\n",
      "Trimmed messages:\n",
      "  system: You are a math tutor. Be concise....\n",
      "  human: What is the square root of 144?...\n",
      "  ai: The square root of 144 is 12....\n",
      "  human: What is 50 - 18?...\n",
      "  ai: 50 - 18 = 32...\n"
     ]
    }
   ],
   "source": [
    "# Trim to keep recent conversation\n",
    "trimmed_messages = trim_messages(\n",
    "    messages,\n",
    "    max_tokens=5,\n",
    "    strategy=\"last\",\n",
    "    token_counter=len,  # Simple counter for demo\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    ")\n",
    "\n",
    "print(f\"Original: {len(messages)} messages\")\n",
    "print(f\"Trimmed: {len(trimmed_messages)} messages\")\n",
    "print(f\"\\nTrimmed messages:\")\n",
    "for msg in trimmed_messages:\n",
    "    print(f\"  {msg.type}: {msg.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conversation Summarization (Long-term Memory)\n",
    "\n",
    "For long conversations, periodically summarize older messages to save tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation has 8 messages\n"
     ]
    }
   ],
   "source": [
    "# Simulate a longer conversation\n",
    "conversation_messages = []\n",
    "\n",
    "conversation_pairs = [\n",
    "    (\"What are quantum computers?\", \"Quantum computers use quantum bits or qubits that can be in superposition...\"),\n",
    "    (\"How do they differ from classical computers?\", \"Classical computers use binary bits while quantum computers leverage quantum mechanics...\"),\n",
    "    (\"What are practical applications?\", \"Applications include cryptography, drug discovery, optimization problems...\"),\n",
    "    (\"Are they commercially available?\", \"Yes, companies like IBM and Google offer cloud access to quantum computers...\"),\n",
    "]\n",
    "\n",
    "for user_msg, ai_msg in conversation_pairs:\n",
    "    conversation_messages.append(HumanMessage(content=user_msg))\n",
    "    conversation_messages.append(AIMessage(content=ai_msg))\n",
    "\n",
    "print(f\"Conversation has {len(conversation_messages)} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation Summary:\n",
      "In the conversation, the HUMAN inquired about quantum computers, learning that they use qubits in superposition, unlike classical computers which use binary bits. The HUMAN then asked about practical applications, to which the AI responded with examples like cryptography, drug discovery, and optimization problems. Finally, the HUMAN asked if quantum computers are commercially available, and the AI confirmed that companies such as IBM and Google offer cloud access to them.\n",
      "\n",
      "Original: ~452 characters\n",
      "Summary: ~476 characters\n"
     ]
    }
   ],
   "source": [
    "# Create a summary\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that summarizes conversations.\"),\n",
    "    (\"user\", \"Please provide a concise summary of the following conversation:\\n\\n{conversation}\"),\n",
    "])\n",
    "\n",
    "# Format conversation as text\n",
    "conversation_text = \"\\n\".join([\n",
    "    f\"{msg.type.upper()}: {msg.content}\" \n",
    "    for msg in conversation_messages\n",
    "])\n",
    "\n",
    "summary_chain = summary_prompt | llm\n",
    "summary_response = summary_chain.invoke({\"conversation\": conversation_text})\n",
    "\n",
    "print(\"\\nConversation Summary:\")\n",
    "print(summary_response.content)\n",
    "print(f\"\\nOriginal: ~{sum(len(msg.content) for msg in conversation_messages)} characters\")\n",
    "print(f\"Summary: ~{len(summary_response.content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sliding Window Pattern\n",
    "\n",
    "Keep only the N most recent exchanges for predictable memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 5 exchanges, memory contains 6 messages:\n",
      "  human: This is question number 3\n",
      "  ai: This is answer number 3\n",
      "  human: This is question number 4\n",
      "  ai: This is answer number 4\n",
      "  human: This is question number 5\n",
      "  ai: This is answer number 5\n"
     ]
    }
   ],
   "source": [
    "class SlidingWindowMemory:\n",
    "    \"\"\"Memory that keeps only the N most recent exchanges.\"\"\"\n",
    "\n",
    "    def __init__(self, max_exchanges=3):\n",
    "        self.max_exchanges = max_exchanges\n",
    "        self.messages = []\n",
    "\n",
    "    def add_exchange(self, human_msg: str, ai_msg: str):\n",
    "        self.messages.append(HumanMessage(content=human_msg))\n",
    "        self.messages.append(AIMessage(content=ai_msg))\n",
    "\n",
    "        # Keep only last N exchanges (2 messages per exchange)\n",
    "        max_msgs = self.max_exchanges * 2\n",
    "        if len(self.messages) > max_msgs:\n",
    "            self.messages = self.messages[-max_msgs:]\n",
    "\n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "\n",
    "    def clear(self):\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "# Test it\n",
    "sliding_memory = SlidingWindowMemory(max_exchanges=3)\n",
    "\n",
    "for i in range(5):\n",
    "    sliding_memory.add_exchange(\n",
    "        f\"This is question number {i + 1}\", f\"This is answer number {i + 1}\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"After 5 exchanges, memory contains {len(sliding_memory.get_messages())} messages:\"\n",
    ")\n",
    "for msg in sliding_memory.get_messages():\n",
    "    print(f\"  {msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Memory Pattern Comparison:**\n",
    "\n",
    "| Pattern | Best For | Pros | Cons |\n",
    "|---------|----------|------|------|\n",
    "| Manual Message History | Simple use cases | Full control, straightforward | Manual management |\n",
    "| RunnableWithMessageHistory | Session-based apps | Automatic session management | Needs storage backend |\n",
    "| Message Trimming | Token management | Predictable usage | May lose context |\n",
    "| Summarization | Long conversations | Bounded memory | Extra LLM calls, loses detail |\n",
    "| Sliding Window | Production chatbots | Consistent performance | Limited history |\n",
    "\n",
    "**Recommended Approaches:**\n",
    "- **Short conversations**: Manual history or RunnableWithMessageHistory\n",
    "- **Long sessions**: Combine trimming + summarization\n",
    "- **Production**: RunnableWithMessageHistory with persistent storage\n",
    "\n",
    "For more details, see the [LangChain memory documentation](https://python.langchain.com/docs/how_to/chatbots_memory/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
