{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Memory Patterns with Amazon Nova\n",
    "\n",
    "This notebook demonstrates modern memory patterns for maintaining conversation context with Nova models.\n",
    "\n",
    "**For current recommendations, see:**\n",
    "- [Short-term memory](https://python.langchain.com/docs/how_to/chatbots_memory/)\n",
    "- [Long-term memory](https://python.langchain.com/docs/how_to/chatbots_long_term_memory/)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure you have set your environment variables:\n",
    "```bash\n",
    "export NOVA_API_KEY=your-api-key\n",
    "export NOVA_BASE_URL=\"https://api.nova.amazon.com/v1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NOVA_API_KEY=<YOUR-API-KEY>\n",
    "%env NOVA_BASE_URL=https://api.nova.amazon.com/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_amazon_nova import ChatAmazonNova\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatAmazonNova(\n",
    "    model=\"nova-pro-v1\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Manual Message History\n",
    "\n",
    "The simplest approach: manually manage a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with system message\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that remembers context.\")\n",
    "]\n",
    "\n",
    "# Turn 1: User introduces themselves\n",
    "messages.append(HumanMessage(content=\"Hi! My name is Alice and I'm learning Python.\"))\n",
    "response1 = llm.invoke(messages)\n",
    "messages.append(AIMessage(content=response1.content))\n",
    "\n",
    "print(\"Turn 1:\")\n",
    "print(f\"User: Hi! My name is Alice and I'm learning Python.\")\n",
    "print(f\"Assistant: {response1.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2: Ask about their name (tests memory)\n",
    "messages.append(HumanMessage(content=\"What's my name and what am I learning?\"))\n",
    "response2 = llm.invoke(messages)\n",
    "messages.append(AIMessage(content=response2.content))\n",
    "\n",
    "print(\"Turn 2:\")\n",
    "print(f\"User: What's my name and what am I learning?\")\n",
    "print(f\"Assistant: {response2.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check message history\n",
    "print(f\"Total messages in history: {len(messages)}\")\n",
    "for i, msg in enumerate(messages):\n",
    "    print(f\"{i + 1}. {msg.type}: {msg.content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RunnableWithMessageHistory (Recommended)\n",
    "\n",
    "Modern approach using `RunnableWithMessageHistory` for session-based conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session store\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First conversation\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi, I'm Bob from Seattle\")]},\n",
    "    config={\"configurable\": {\"session_id\": \"user_123\"}},\n",
    ")\n",
    "\n",
    "print(f\"User: Hi, I'm Bob from Seattle\")\n",
    "print(f\"Assistant: {response.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second conversation - remembers context\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's my name and where am I from?\")]},\n",
    "    config={\"configurable\": {\"session_id\": \"user_123\"}},\n",
    ")\n",
    "\n",
    "print(f\"User: What's my name and where am I from?\")\n",
    "print(f\"Assistant: {response.content}\\n\")\n",
    "\n",
    "# Check session history\n",
    "history = get_session_history(\"user_123\")\n",
    "print(f\"Session has {len(history.messages)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Message Trimming (Context Window Management)\n",
    "\n",
    "Use `trim_messages` to manage the context window and stay within token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [SystemMessage(content=\"You are a math tutor. Be concise.\")]\n",
    "\n",
    "# Simulate multiple questions\n",
    "questions = [\n",
    "    \"What is 15 + 27?\",\n",
    "    \"What is 8 * 9?\",\n",
    "    \"What is 100 / 4?\",\n",
    "    \"What is 12 squared?\",\n",
    "    \"What is the square root of 144?\",\n",
    "    \"What is 50 - 18?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    messages.append(HumanMessage(content=question))\n",
    "    response = llm.invoke(messages)\n",
    "    messages.append(AIMessage(content=response.content))\n",
    "    print(f\"Q{i}: {question} -> A{i}: {response.content}\")\n",
    "\n",
    "print(f\"\\nTotal messages: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim to keep recent conversation\n",
    "trimmed_messages = trim_messages(\n",
    "    messages,\n",
    "    max_tokens=100,\n",
    "    strategy=\"last\",\n",
    "    token_counter=len,  # Simple counter for demo\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    ")\n",
    "\n",
    "print(f\"Original: {len(messages)} messages\")\n",
    "print(f\"Trimmed: {len(trimmed_messages)} messages\")\n",
    "print(f\"\\nTrimmed messages:\")\n",
    "for msg in trimmed_messages:\n",
    "    print(f\"  {msg.type}: {msg.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conversation Summarization (Long-term Memory)\n",
    "\n",
    "For long conversations, periodically summarize older messages to save tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a longer conversation\n",
    "conversation_messages = []\n",
    "\n",
    "conversation_pairs = [\n",
    "    (\"What are quantum computers?\", \"Quantum computers use quantum bits or qubits that can be in superposition...\"),\n",
    "    (\"How do they differ from classical computers?\", \"Classical computers use binary bits while quantum computers leverage quantum mechanics...\"),\n",
    "    (\"What are practical applications?\", \"Applications include cryptography, drug discovery, optimization problems...\"),\n",
    "    (\"Are they commercially available?\", \"Yes, companies like IBM and Google offer cloud access to quantum computers...\"),\n",
    "]\n",
    "\n",
    "for user_msg, ai_msg in conversation_pairs:\n",
    "    conversation_messages.append(HumanMessage(content=user_msg))\n",
    "    conversation_messages.append(AIMessage(content=ai_msg))\n",
    "\n",
    "print(f\"Conversation has {len(conversation_messages)} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Provide a concise summary of the conversation below.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "summary_chain = summary_prompt | llm\n",
    "summary_response = summary_chain.invoke({\"messages\": conversation_messages})\n",
    "\n",
    "print(\"\\nConversation Summary:\")\n",
    "print(summary_response.content)\n",
    "print(f\"\\nOriginal: ~{sum(len(msg.content) for msg in conversation_messages)} characters\")\n",
    "print(f\"Summary: ~{len(summary_response.content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sliding Window Pattern\n",
    "\n",
    "Keep only the N most recent exchanges for predictable memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowMemory:\n",
    "    \"\"\"Memory that keeps only the N most recent exchanges.\"\"\"\n",
    "\n",
    "    def __init__(self, max_exchanges=3):\n",
    "        self.max_exchanges = max_exchanges\n",
    "        self.messages = []\n",
    "\n",
    "    def add_exchange(self, human_msg: str, ai_msg: str):\n",
    "        self.messages.append(HumanMessage(content=human_msg))\n",
    "        self.messages.append(AIMessage(content=ai_msg))\n",
    "\n",
    "        # Keep only last N exchanges (2 messages per exchange)\n",
    "        max_msgs = self.max_exchanges * 2\n",
    "        if len(self.messages) > max_msgs:\n",
    "            self.messages = self.messages[-max_msgs:]\n",
    "\n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "\n",
    "    def clear(self):\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "# Test it\n",
    "sliding_memory = SlidingWindowMemory(max_exchanges=3)\n",
    "\n",
    "for i in range(5):\n",
    "    sliding_memory.add_exchange(\n",
    "        f\"This is question number {i + 1}\", f\"This is answer number {i + 1}\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"After 5 exchanges, memory contains {len(sliding_memory.get_messages())} messages:\"\n",
    ")\n",
    "for msg in sliding_memory.get_messages():\n",
    "    print(f\"  {msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Memory Pattern Comparison:**\n",
    "\n",
    "| Pattern | Best For | Pros | Cons |\n",
    "|---------|----------|------|------|\n",
    "| Manual Message History | Simple use cases | Full control, straightforward | Manual management |\n",
    "| RunnableWithMessageHistory | Session-based apps | Automatic session management | Needs storage backend |\n",
    "| Message Trimming | Token management | Predictable usage | May lose context |\n",
    "| Summarization | Long conversations | Bounded memory | Extra LLM calls, loses detail |\n",
    "| Sliding Window | Production chatbots | Consistent performance | Limited history |\n",
    "\n",
    "**Recommended Approaches:**\n",
    "- **Short conversations**: Manual history or RunnableWithMessageHistory\n",
    "- **Long sessions**: Combine trimming + summarization\n",
    "- **Production**: RunnableWithMessageHistory with persistent storage\n",
    "\n",
    "For more details, see the [LangChain memory documentation](https://python.langchain.com/docs/how_to/chatbots_memory/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
