{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template Patterns with Amazon Nova\n",
    "\n",
    "This notebook demonstrates various prompt engineering patterns using LangChain templates.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NOVA_API_KEY=<YOUR-API-KEY>\n",
    "%env NOVA_BASE_URL=https://api.nova.amazon.com/v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_amazon_nova import ChatAmazonNova\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatAmazonNova(model=\"nova-pro-v1\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple String Template\n",
    "\n",
    "Basic template with variable substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_template(\n",
    "    \"You are an expert in {subject}. Explain {concept} in simple terms.\"\n",
    ")\n",
    "\n",
    "chain = template | llm\n",
    "result = chain.invoke({\"subject\": \"physics\", \"concept\": \"quantum entanglement\"})\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chat Template with System Message\n",
    "\n",
    "Use tuples to specify different message roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a {role}. Keep responses {style}.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = chat_template | llm\n",
    "result = chain.invoke(\n",
    "    {\n",
    "        \"role\": \"pirate\",\n",
    "        \"style\": \"brief and in character\",\n",
    "        \"input\": \"What's the weather like?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Few-Shot Examples\n",
    "\n",
    "Provide examples to guide the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define examples\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"hot\", \"output\": \"cold\"},\n",
    "]\n",
    "\n",
    "# Create example template\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create few-shot prompt\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Combine into final prompt\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are providing antonyms.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | llm\n",
    "result = chain.invoke({\"input\": \"light\"})\n",
    "\n",
    "print(f\"Input: 'light'\")\n",
    "print(f\"Output: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Message Placeholder for Chat History\n",
    "\n",
    "Use `MessagesPlaceholder` to inject conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_history = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create chat history\n",
    "history = [\n",
    "    HumanMessage(content=\"My name is Alice\"),\n",
    "    AIMessage(content=\"Nice to meet you, Alice!\"),\n",
    "]\n",
    "\n",
    "chain = prompt_with_history | llm\n",
    "result = chain.invoke({\"chat_history\": history, \"input\": \"What's my name?\"})\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Variable Complex Template\n",
    "\n",
    "Templates can have many variables and conditional logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a {profession} with {years} years of experience.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Context: {context}\\n\\nQuestion: {question}\\n\\nPlease answer in {style} style.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = complex_template | llm\n",
    "result = chain.invoke(\n",
    "    {\n",
    "        \"profession\": \"software architect\",\n",
    "        \"years\": 15,\n",
    "        \"context\": \"We're building a high-traffic e-commerce platform\",\n",
    "        \"question\": \"Should we use microservices or monolith?\",\n",
    "        \"style\": \"concise\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Partial Variables\n",
    "\n",
    "Pre-fill some template variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create template with partial\n",
    "template = ChatPromptTemplate.from_template(\"Translate '{text}' to {language}\")\n",
    "\n",
    "# Partially fill the template\n",
    "spanish_template = template.partial(language=\"Spanish\")\n",
    "french_template = template.partial(language=\"French\")\n",
    "\n",
    "# Now only need to provide text\n",
    "spanish_chain = spanish_template | llm\n",
    "french_chain = french_template | llm\n",
    "\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "spanish = spanish_chain.invoke({\"text\": text})\n",
    "french = french_chain.invoke({\"text\": text})\n",
    "\n",
    "print(f\"Spanish: {spanish.content}\")\n",
    "print(f\"French: {french.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dynamic Few-Shot Selection\n",
    "\n",
    "Select relevant examples based on input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library of examples\n",
    "example_library = {\n",
    "    \"math\": [\n",
    "        {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "        {\"input\": \"5*3\", \"output\": \"15\"},\n",
    "    ],\n",
    "    \"grammar\": [\n",
    "        {\"input\": \"their there\", \"output\": \"they're\"},\n",
    "        {\"input\": \"your you're\", \"output\": \"you're\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def create_prompt_for_category(category: str):\n",
    "    \"\"\"Create a prompt with relevant examples.\"\"\"\n",
    "    examples = example_library.get(category, [])\n",
    "\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"ai\", \"{output}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    few_shot = FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=examples,\n",
    "    )\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", f\"You are helping with {category} problems.\"),\n",
    "            few_shot,\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# Use math examples\n",
    "math_prompt = create_prompt_for_category(\"math\")\n",
    "math_chain = math_prompt | llm\n",
    "\n",
    "result = math_chain.invoke({\"input\": \"10-3\"})\n",
    "print(f\"Math result: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Template Composition\n",
    "\n",
    "Build complex prompts from smaller reusable pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable components\n",
    "system_template = \"You are an AI assistant specialized in {domain}.\"\n",
    "context_template = \"Context information:\\n{context}\"\n",
    "question_template = \"Question: {question}\"\n",
    "\n",
    "# Compose them\n",
    "full_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", f\"{context_template}\\n\\n{question_template}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = full_template | llm\n",
    "result = chain.invoke(\n",
    "    {\n",
    "        \"domain\": \"biology\",\n",
    "        \"context\": \"Photosynthesis is the process by which plants convert light into energy.\",\n",
    "        \"question\": \"What is the primary product of photosynthesis?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Templates with Advanced Parameters\n",
    "\n",
    "Combine prompt engineering with API parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different models for different tasks\n",
    "analytical_llm = ChatAmazonNova(\n",
    "    model=\"nova-pro-v1\", temperature=0.2, reasoning_effort=\"high\", max_tokens=200\n",
    ")\n",
    "\n",
    "creative_llm = ChatAmazonNova(\n",
    "    model=\"nova-pro-v1\", temperature=0.9, top_p=0.95, max_tokens=200\n",
    ")\n",
    "\n",
    "# Same template, different parameters\n",
    "template = ChatPromptTemplate.from_template(\"Write about {topic}\")\n",
    "\n",
    "analytical_chain = template | analytical_llm\n",
    "creative_chain = template | creative_llm\n",
    "\n",
    "topic = \"the future of AI\"\n",
    "\n",
    "print(\"Analytical (low temp, high reasoning):\")\n",
    "print(analytical_chain.invoke({\"topic\": topic}).content)\n",
    "\n",
    "print(\"\\nCreative (high temp, high top-p):\")\n",
    "print(creative_chain.invoke({\"topic\": topic}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Prompt Template Patterns:**\n",
    "\n",
    "| Pattern | Use Case | Benefits |\n",
    "|---------|----------|----------|\n",
    "| Simple Template | Basic variable substitution | Easy to use, readable |\n",
    "| Chat Template | Role-based messaging | Clear conversation structure |\n",
    "| Few-Shot | Provide examples | Guides model behavior |\n",
    "| Message Placeholder | Inject chat history | Maintains conversation context |\n",
    "| Multi-Variable | Complex prompts | Flexible, reusable |\n",
    "| Partial Variables | Pre-filled templates | Reduces repetition |\n",
    "| Dynamic Selection | Context-aware examples | Intelligent prompt adaptation |\n",
    "| Composition | Build from components | Modular, maintainable |\n",
    "\n",
    "**Best Practices:**\n",
    "- **Be Specific**: Clear variables names and instructions\n",
    "- **Provide Examples**: Few-shot learning improves accuracy\n",
    "- **Use Structure**: System/Human/AI roles for clarity\n",
    "- **Test Variations**: Try different phrasings\n",
    "- **Compose Reusably**: Build libraries of template components\n",
    "- **Version Control**: Track prompt changes like code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
