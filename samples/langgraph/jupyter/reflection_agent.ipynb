{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection Pattern with LangGraph and ChatAmazonNova\n",
    "\n",
    "This notebook demonstrates an agent that critiques and improves its own outputs through iterative reflection.\n",
    "\n",
    "## Process Flow\n",
    "1. **Generate**: Create initial content\n",
    "2. **Reflect**: Critique the output\n",
    "3. **Generate**: Revise based on critique\n",
    "4. **Repeat**: Until approved or max iterations reached\n",
    "\n",
    "## Key Concepts\n",
    "- Self-improvement loops\n",
    "- Quality iteration\n",
    "- Conditional termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NOVA_API_KEY=<YOUR-API-KEY>\n",
    "%env NOVA_BASE_URL=https://api.nova.amazon.com/v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from langchain_amazon_nova import ChatAmazonNova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionState(TypedDict):\n",
    "    \"\"\"State for the reflection loop.\"\"\"\n",
    "    messages: List\n",
    "    iterations: int\n",
    "    max_iterations: int\n",
    "    initial_query: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Generator and Reflector\n",
    "\n",
    "The generator creates content, the reflector critiques it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(llm):\n",
    "    \"\"\"Create the content generator.\"\"\"\n",
    "    \n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a helpful assistant that generates content based on user requests.\n",
    "        If you receive critique or feedback, revise your previous response to address the concerns.\"\"\"\n",
    "    )\n",
    "    \n",
    "    def generate(state: ReflectionState) -> dict:\n",
    "        messages = [system_message] + state[\"messages\"]\n",
    "        response = llm.invoke(messages)\n",
    "        \n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + [response],\n",
    "            \"iterations\": state[\"iterations\"] + 1,\n",
    "        }\n",
    "    \n",
    "    return generate\n",
    "\n",
    "\n",
    "def create_reflector(llm):\n",
    "    \"\"\"Create the content reflector/critic.\"\"\"\n",
    "    \n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a thoughtful critic. Review the assistant's response and provide specific,\n",
    "        constructive feedback on how it could be improved. Consider:\n",
    "        - Accuracy and completeness\n",
    "        - Clarity and structure\n",
    "        - Tone and style appropriateness\n",
    "        - Any missing important details\n",
    "        \n",
    "        If the response is excellent and needs no improvements, respond with exactly: \"APPROVED\"\n",
    "        Otherwise, provide specific suggestions for improvement.\"\"\"\n",
    "    )\n",
    "    \n",
    "    def reflect(state: ReflectionState) -> dict:\n",
    "        last_response = state[\"messages\"][-1]\n",
    "        \n",
    "        critique_messages = [\n",
    "            system_message,\n",
    "            HumanMessage(content=f\"Original request: {state['initial_query']}\"),\n",
    "            HumanMessage(content=f\"Assistant's response: {last_response.content}\"),\n",
    "        ]\n",
    "        \n",
    "        response = llm.invoke(critique_messages)\n",
    "        \n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + [\n",
    "                HumanMessage(content=f\"Critique: {response.content}\")\n",
    "            ],\n",
    "        }\n",
    "    \n",
    "    return reflect\n",
    "\n",
    "\n",
    "print(\"Generator and reflector defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Routing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: ReflectionState) -> str:\n",
    "    \"\"\"Decide whether to continue reflecting or end.\"\"\"\n",
    "    # Check if we've hit max iterations\n",
    "    if state[\"iterations\"] >= state[\"max_iterations\"]:\n",
    "        return \"end\"\n",
    "    \n",
    "    # Check if approved\n",
    "    if state[\"messages\"]:\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if isinstance(last_message, HumanMessage) and \"APPROVED\" in last_message.content:\n",
    "            return \"end\"\n",
    "    \n",
    "    # Alternate between generate and reflect\n",
    "    if state[\"iterations\"] % 2 == 1:\n",
    "        return \"reflect\"\n",
    "    else:\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Reflection Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reflection_graph(llm, max_iterations: int = 3):\n",
    "    \"\"\"Create the reflection agent graph.\"\"\"\n",
    "    \n",
    "    generator = create_generator(llm)\n",
    "    reflector = create_reflector(llm)\n",
    "    \n",
    "    workflow = StateGraph(ReflectionState)\n",
    "    \n",
    "    workflow.add_node(\"generate\", generator)\n",
    "    workflow.add_node(\"reflect\", reflector)\n",
    "    \n",
    "    workflow.add_edge(START, \"generate\")\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"generate\",\n",
    "        should_continue,\n",
    "        {\"reflect\": \"reflect\", \"end\": END}\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"reflect\",\n",
    "        should_continue,\n",
    "        {\"generate\": \"generate\", \"end\": END}\n",
    "    )\n",
    "    \n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "llm = ChatAmazonNova(\n",
    "    model=\"nova-pro-v1\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Create reflection agent\n",
    "max_iterations = 4\n",
    "agent = create_reflection_graph(llm, max_iterations)\n",
    "\n",
    "print(f\"Reflection agent initialized with max {max_iterations} iterations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Short Essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Write a short essay on the importance of renewable energy\"\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=query)],\n",
    "    \"iterations\": 0,\n",
    "    \"max_iterations\": max_iterations,\n",
    "    \"initial_query\": query,\n",
    "})\n",
    "\n",
    "# Get final output (last AI message)\n",
    "final_output = None\n",
    "for msg in reversed(result[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage):\n",
    "        final_output = msg.content\n",
    "        break\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=== Final Output ===\")\n",
    "print(final_output)\n",
    "print(f\"\\n(Completed in {result['iterations']} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Explain a Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain machine learning to a 12-year-old\"\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=query)],\n",
    "    \"iterations\": 0,\n",
    "    \"max_iterations\": max_iterations,\n",
    "    \"initial_query\": query,\n",
    "})\n",
    "\n",
    "# Get final output\n",
    "final_output = None\n",
    "for msg in reversed(result[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage):\n",
    "        final_output = msg.content\n",
    "        break\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=== Final Output ===\")\n",
    "print(final_output)\n",
    "print(f\"\\n(Completed in {result['iterations']} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Reflection Process\n",
    "\n",
    "See the full generate-reflect-revise cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total messages: {len(result['messages'])}\\n\")\n",
    "print(\"Reflection process:\\n\")\n",
    "\n",
    "for i, msg in enumerate(result['messages']):\n",
    "    if msg.type == \"human\":\n",
    "        if \"Critique:\" in msg.content:\n",
    "            print(f\"{i+1}. [Critique] {msg.content[:100]}...\")\n",
    "        else:\n",
    "            print(f\"{i+1}. [User] {msg.content[:100]}...\")\n",
    "    elif msg.type == \"ai\":\n",
    "        print(f\"{i+1}. [Generated] {msg.content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Your Own Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the query below\n",
    "your_query = \"Write a professional email declining a meeting invitation politely\"\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=your_query)],\n",
    "    \"iterations\": 0,\n",
    "    \"max_iterations\": max_iterations,\n",
    "    \"initial_query\": your_query,\n",
    "})\n",
    "\n",
    "# Get final output\n",
    "final_output = None\n",
    "for msg in reversed(result[\"messages\"]):\n",
    "    if isinstance(msg, AIMessage):\n",
    "        final_output = msg.content\n",
    "        break\n",
    "\n",
    "print(f\"Query: {your_query}\\n\")\n",
    "print(\"=== Final Output ===\")\n",
    "print(final_output)\n",
    "print(f\"\\n(Completed in {result['iterations']} iterations)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
